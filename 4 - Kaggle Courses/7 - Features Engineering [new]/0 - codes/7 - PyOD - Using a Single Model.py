"""
		**********
		** PyOD **
		**********

- Angle-Based Outlier Detection (ABOD):

	/ It considers the relationship between each point and its neighbor(s). It does not consider the relationships among these neighbors. The variance of its weighted cosine scores to all neighbors could be viewed as the outlying score;

	/ ABOD performs well on multi-dimensional data;

	/ PyOD provides two different versions of ABOD:

		// Fast ABOD: Uses k-nearest neighbors to approximate;
		// Original ABOD: Considers all training points with high-time complexity.

-*-*-*-*-

- k-Nearest Neighbors Detector:

	/ For any data point, the distance to its kth nearest neighbor could be viewed as the outlying score;

	/ PyOD supports three kNN detectors:

		// Largest: Uses the distance of the kth neighbor as the outlier score;
		// Mean: Uses the average of all k neighbors as the outlier score;
		// Median: Uses the median of the distance to k neighbors as the outlier score.

-*-*-*-*-

- Isolation Forest:

	/ It uses the scikit-learn library internally. In this method, data partitioning is done using a set of trees. Isolation Forest provides an anomaly score looking at how isolated the point is in the structure. The anomaly score is then used to identify outliers from normal observations;

	/ Isolation Forest performs well on multi-dimensional data.

-*-*-*-*-

- Histogram-based Outlier Detection:

	/ It is an efficient unsupervised method which assumes the feature independence and calculates the outlier score by building histograms;

	/ It is much faster than multivariate approaches, but at the cost of less precision.

-*-*-*-*-

- Feature Bagging:

	/ A feature bagging detector fits a number of base detectors on various sub-samples of the dataset. It uses averaging or other combination methods to improve the prediction accuracy;

	/ By default, Local Outlier Factor (LOF) is used as the base estimator. However, any estimator could be used as the base estimator, such as kNN and ABOD;

	/ Feature bagging first constructs n sub-samples by randomly selecting a subset of features. This brings out the diversity of base estimators. Finally, the prediction score is generated by averaging or taking the maximum of all base detectors.

-*-*-*-*-

- Clustering Based Local Outlier Factor:

	/ It classifies the data into small clusters and large clusters. The anomaly score is then calculated based on the size of the cluster the point belongs to, as well as the distance to the nearest large cluster.

-*-*-*-*-

- Extra Utilities provided by PyOD:

	/ A function "generate_data" can be used to generate random data with outliers. Inliers data is generated by a multivariate Gaussian distribution and outliers are generated by a uniform distribution;

	/ We can provide our own values of outliers fraction and the total number of samples that we want in our dataset. We will use this utility function to create data in the implementation part.
"""

from pyod.models.abod import ABOD
from pyod.models.cblof import CBLOF
from pyod.models.iforest import IForest
from pyod.models.knn import KNN
from pyod.models.lof import LOF
from pyod.models.ocsvm import OCSVM
from pyod.models.pca import PCA

# 0 - Defining 'outlier_fraction'
# PS.: outlier fraction is optional to be defined
from random import randrange
out_frac = randrange(0, 45) / 100

# 1 - Defining Methods
rs = np.random.RandomState(42)

clf = { 
    'Angle-based Outlier Detector (ABOD)': ABOD(contamination=out_frac),
    'Cluster-based Local Outlier Factor (CBLOF)': CBLOF(contamination=out_frac,check_estimator=False, random_state=rs),
    'Isolation Forest': IForest(contamination=out_frac,random_state=rs),
    'K Nearest Neighbors (KNN)': KNN(contamination=out_frac, method='largest', n_neighbors=5, n_jobs=4),
    'Average KNN': KNN(method='mean', contamination=out_frac),
    'Local Outlier Factor (LOF)':LOF(n_neighbors=35, contamination=out_frac),
    'One-class SVM (OCSVM)': OCSVM(contamination=out_frac),
    'Principal Component Analysis (PCA)': PCA(contamination=out_frac, random_state=rs),
} 

# 2 - Training PyOD
clf_name = 'K Nearest Neighbors (KNN)'
clf['K Nearest Neighbors (KNN)'].fit(X_train, n_jobs=2)

# 3 - Predictions and Scores

# getting the prediction label and outlier scores of the training data
y_train_pred = clf[clf_name].labels_  # binary labels (0: inliers, 1: outliers)
y_train_scores = clf[clf_name].decision_scores_  # raw outlier scores (distances)
y_train_number_outliers = np.unique(y_train_pred, return_counts=True) # number of outliers

y_test_pred = clf[clf_name].predict(X_test)  # outlier labels (0 or 1)
y_test_scores = clf[clf_name].decision_function(X_test)  # outlier scores
y_test_number_outliers = np.unique(y_test_pred, return_counts=True) # number of outliers

# prediction confidence
# outlier labels (0 or 1) and confidence in the range of [0,1]
y_test_pred, y_test_pred_confidence = clf[clf_name].predict(X_test, return_confidence=True)

# 3 - Evaluations
from pyod.utils.data import evaluate_print

print("\nOn Training Data:")
evaluate_print(clf_name, y_train, y_train_scores)
print("\nOn Test Data:")
evaluate_print(clf_name, y_test, y_test_scores)

visualize(clf_name, X_train, y_train, X_test, y_test, y_train_pred,
    y_test_pred, show_figure=True, save_figure=False)

# 4 - Getting Outliers Indexes in the dataset
outliers_indexes = [
	index for index in range(0, len(y_train_pred))
	if y_train_pred[index] == 1
]

outlier_rows = y_train.iloc[outliers_indexes]
outlier_rows